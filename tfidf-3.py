import os
import time
import numpy as np
from tqdm import tqdm  # å¯¼å…¥è¿›åº¦æ¡æ¨¡å—
from scipy.sparse import vstack, csr_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import Normalizer

# ğŸ“‚ è®¾å®šæ–‡ä»¶è·¯å¾„
data_dir = "/Users/wangeiden/Desktop/wikipedia_processed"
output_dir = "/Users/wangeiden/Desktop/tfidf_output"  # å­˜å‚¨è¾“å‡ºæ–‡ä»¶çš„æ–‡ä»¶å¤¹
timestamp = time.strftime("%Y%m%d_%H%M%S")
log_file = f"{output_dir}/tfidf_log_{timestamp}.txt"

# åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
os.makedirs(output_dir, exist_ok=True)

file_list = sorted([f for f in os.listdir(data_dir) if f.endswith(".arrow.txt")])

# åˆ†æ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹å¤„ç†7ä¸ªæ–‡ä»¶
batch_size = 7
num_batches = len(file_list) // batch_size + (1 if len(file_list) % batch_size > 0 else 0)

# ç”¨äºè®°å½•å¤„ç†çš„æ–‡æ¡£æ•°å’Œç‰¹å¾æ•°
total_documents = 0
total_features = 0

# é€æ‰¹å¤„ç†æ–‡ä»¶
for batch_num in tqdm(range(num_batches), desc="Processing Batches", unit="batch"):
    batch_start = batch_num * batch_size
    batch_end = min((batch_num + 1) * batch_size, len(file_list))
    current_files = file_list[batch_start:batch_end]

    # è¯»å–æ•°æ®å¹¶è®¡ç®— TF-IDF
    documents = []
    print(f"\nğŸ“‚ æ­£åœ¨å¤„ç†ç¬¬ {batch_num + 1} æ‰¹æ–‡ä»¶ï¼Œå…± {len(current_files)} ä¸ªæ–‡ä»¶...")
    for file_name in current_files:
        file_path = os.path.join(data_dir, file_name)
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    documents.append(line.strip()[:1000])  # åªå–æ¯è¡Œçš„å‰1000ä¸ªå­—ç¬¦
        except Exception as e:
            print(f"âŒ é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ {file_name} - {e}")
    
    # è®¡ç®— TF-IDF
    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=0.85, min_df=5)
    tfidf_matrix = vectorizer.fit_transform(documents)
    
    # å½’ä¸€åŒ–
    normalizer = Normalizer()
    tfidf_matrix = normalizer.transform(tfidf_matrix)
    
    # ä¿å­˜ç»“æœåˆ°è¾“å‡ºæ–‡ä»¶å¤¹
    output_file = f"{output_dir}/tfidf_batch_{batch_num + 1}_{timestamp}.npz"
    from scipy.sparse import save_npz
    save_npz(output_file, tfidf_matrix)

    # è®°å½•æ–‡æ¡£å’Œç‰¹å¾æ•°
    total_documents += len(documents)
    total_features = tfidf_matrix.shape[1]

    print(f"âœ… æ‰¹æ¬¡ {batch_num + 1} å¤„ç†å®Œæˆï¼ŒTF-IDF çŸ©é˜µå¤§å°: {tfidf_matrix.shape}. ç»“æœä¿å­˜è‡³ {output_file}.")

    # æ¸…ç†å†…å­˜
    del documents
    del tfidf_matrix
    del vectorizer
    del normalizer
    import gc
    gc.collect()  # æ˜¾å¼è°ƒç”¨åƒåœ¾å›æ”¶

# è®°å½•æ—¥å¿—
with open(log_file, "w", encoding="utf-8") as log_f:
    log_f.write(f"TF-IDF è®¡ç®—å®Œæˆï¼Œå…± {total_documents} ä¸ªæ–‡æ¡£ï¼Œ{total_features} ä¸ªç‰¹å¾ã€‚\n")
    log_f.write(f"å¤„ç†çš„æ–‡ä»¶å…± {len(file_list)} ä¸ªæ–‡ä»¶ã€‚\n")
    log_f.write(f"æ‰€æœ‰è¾“å‡ºç»“æœå·²ä¿å­˜è‡³ {output_dir} æ–‡ä»¶å¤¹ã€‚\n")

print(f"\nâœ… æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œæ—¥å¿—å·²ä¿å­˜è‡³ {log_file}ã€‚")
