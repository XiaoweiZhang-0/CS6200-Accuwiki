import os
import re
import ssl
import nltk
from datasets import load_dataset
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from tqdm import tqdm

# âœ… è§£å†³ SSL è¯ä¹¦é—®é¢˜
ssl._create_default_https_context = ssl._create_unverified_context

# âœ… ä¸‹è½½ NLTK èµ„æº
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# âœ… **æ–‡ä»¶è·¯å¾„**
input_folder = "/Users/wangeiden/Desktop/wikipedia_"  # è¾“å…¥ `.arrow` æ–‡ä»¶çš„æ–‡ä»¶å¤¹
output_folder = "/Users/wangeiden/Desktop/wikipedia_processed/"  # å¤„ç†åæ–‡ä»¶å­˜æ”¾çš„æ–‡ä»¶å¤¹

# âœ… **åˆ›å»ºå­˜å‚¨ç›®å½•**
os.makedirs(output_folder, exist_ok=True)

# âœ… **åˆå§‹åŒ–å·¥å…·**
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))  # ç”¨ `set()` æé«˜æŸ¥æ‰¾é€Ÿåº¦

# âœ… **è¯æ€§æ˜ å°„ (æé«˜ Lemmatization å‡†ç¡®åº¦)**
def get_wordnet_pos(word):
    tag = pos_tag([word])[0][1][0].upper()  # è·å–POSæ ‡æ³¨çš„é¦–å­—æ¯
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # é»˜è®¤ä½œä¸ºåè¯å¤„ç†

# âœ… **ä¿®æ­£ contractionsï¼ˆé¿å… omens é”™è¯¯ï¼‰**
contractions = {
    "women's": "woman",
    "can't": "cannot",
    "isn't": "is not",
    "doesn't": "does not",
    "won't": "will not",
    "i'm": "i am",
    "you're": "you are",
    "they're": "they are",
    "we're": "we are",
    "it's": "it is",
}

def expand_contractions(text):
    words = text.split()
    words = [contractions[word] if word in contractions else word for word in words]
    return " ".join(words)

# âœ… **æ–‡æœ¬é¢„å¤„ç†**
def preprocess_text(text):
    text = expand_contractions(text)  # **å…ˆå¤„ç† contractions**
    text = text.lower()  # **è½¬å°å†™**
    text = re.sub(r"[^\w\s]", "", text)  # **å»é™¤æ ‡ç‚¹**
    words = word_tokenize(text)  # **åˆ†è¯**
    
    # **ğŸ”¥ è¯å½¢è¿˜åŸ + è¯æ€§æ ‡æ³¨**
    processed_words = [
        lemmatizer.lemmatize(word, get_wordnet_pos(word))  # **ç¡®ä¿ `known` å˜æˆ `know`**
        for word in words if word not in stop_words
    ]
    return " ".join(processed_words)  # **æœ€å¿«æ‹¼æ¥**

# âœ… **è·å–æ‰€æœ‰ `.arrow` æ–‡ä»¶**
arrow_files = [f for f in os.listdir(input_folder) if f.endswith(".arrow")]
if not arrow_files:
    raise ValueError(f"ç›®å½• {input_folder} ä¸­æ²¡æœ‰ Arrow æ–‡ä»¶ï¼")

# âœ… **é€ä¸ªå¤„ç†æ¯ä¸ª `.arrow` æ–‡ä»¶**
for arrow_file in tqdm(arrow_files, desc="ğŸ“‚ å¤„ç†æ–‡ä»¶è¿›åº¦", unit="file"):
    input_path = os.path.join(input_folder, arrow_file)
    output_path = os.path.join(output_folder, f"processed_{arrow_file}.txt")

    print(f"\nğŸ“‚ å¼€å§‹å¤„ç†æ–‡ä»¶: {input_path}")

    try:
        dataset = load_dataset("arrow", data_files=input_path, split="train")

        if "text" in dataset.column_names:
            texts = dataset["text"]
            
            # âœ… **æ‰¹é‡å¤„ç†æ–‡æœ¬**
            processed_texts = [preprocess_text(text) for text in tqdm(texts, desc="âœ… å¤„ç†æ–‡æœ¬", unit="æ¡")]

            # âœ… **ä¿å­˜å¤„ç†åçš„æ–‡æœ¬**
            with open(output_path, "w", encoding="utf-8") as f:
                for raw, processed in zip(texts, processed_texts):
                    f.write(f"åŸå§‹æ–‡æœ¬:\n{raw}\n")
                    f.write(f"å¤„ç†åæ–‡æœ¬:\n{processed}\n")
                    f.write("=" * 100 + "\n")

            print(f"ğŸ‰ **æ–‡ä»¶ {arrow_file} å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {output_path}**")

        else:
            print(f"âš ï¸ æ–‡ä»¶ {arrow_file} ä¸­æ²¡æœ‰ 'text' åˆ—ï¼Œè·³è¿‡ï¼")

    except Exception as e:
        print(f"âŒ å¤„ç† {arrow_file} å¤±è´¥: {e}")

print(f"\nğŸ‰ **æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼**")
print(f"ğŸ“ é¢„å¤„ç†ç»“æœå·²å­˜æ”¾åœ¨: {output_folder}")
import os
import re
import ssl
import nltk
from datasets import load_dataset
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from tqdm import tqdm

# âœ… è§£å†³ SSL è¯ä¹¦é—®é¢˜
ssl._create_default_https_context = ssl._create_unverified_context

# âœ… ä¸‹è½½ NLTK èµ„æº
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# âœ… **æ–‡ä»¶è·¯å¾„**
input_folder = "/Users/wangeiden/Desktop/wikipedia_"  # è¾“å…¥ `.arrow` æ–‡ä»¶çš„æ–‡ä»¶å¤¹
output_folder = "/Users/wangeiden/Desktop/wikipedia_processed/"  # å¤„ç†åæ–‡ä»¶å­˜æ”¾çš„æ–‡ä»¶å¤¹

# âœ… **åˆ›å»ºå­˜å‚¨ç›®å½•**
os.makedirs(output_folder, exist_ok=True)

# âœ… **åˆå§‹åŒ–å·¥å…·**
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))  # ç”¨ `set()` æé«˜æŸ¥æ‰¾é€Ÿåº¦

# âœ… **è¯æ€§æ˜ å°„ (æé«˜ Lemmatization å‡†ç¡®åº¦)**
def get_wordnet_pos(word):
    tag = pos_tag([word])[0][1][0].upper()  # è·å–POSæ ‡æ³¨çš„é¦–å­—æ¯
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # é»˜è®¤ä½œä¸ºåè¯å¤„ç†

# âœ… **ä¿®æ­£ contractionsï¼ˆé¿å… omens é”™è¯¯ï¼‰**
contractions = {
    "women's": "woman",
    "can't": "cannot",
    "isn't": "is not",
    "doesn't": "does not",
    "won't": "will not",
    "i'm": "i am",
    "you're": "you are",
    "they're": "they are",
    "we're": "we are",
    "it's": "it is",
}

def expand_contractions(text):
    words = text.split()
    words = [contractions[word] if word in contractions else word for word in words]
    return " ".join(words)

# âœ… **æ–‡æœ¬é¢„å¤„ç†**
def preprocess_text(text):
    text = expand_contractions(text)  # **å…ˆå¤„ç† contractions**
    text = text.lower()  # **è½¬å°å†™**
    text = re.sub(r"[^\w\s]", "", text)  # **å»é™¤æ ‡ç‚¹**
    words = word_tokenize(text)  # **åˆ†è¯**
    
    # **ğŸ”¥ è¯å½¢è¿˜åŸ + è¯æ€§æ ‡æ³¨**
    processed_words = [
        lemmatizer.lemmatize(word, get_wordnet_pos(word))  # **ç¡®ä¿ `known` å˜æˆ `know`**
        for word in words if word not in stop_words
    ]
    return " ".join(processed_words)  # **æœ€å¿«æ‹¼æ¥**

# âœ… **è·å–æ‰€æœ‰ `.arrow` æ–‡ä»¶**
arrow_files = [f for f in os.listdir(input_folder) if f.endswith(".arrow")]
if not arrow_files:
    raise ValueError(f"ç›®å½• {input_folder} ä¸­æ²¡æœ‰ Arrow æ–‡ä»¶ï¼")

# âœ… **é€ä¸ªå¤„ç†æ¯ä¸ª `.arrow` æ–‡ä»¶**
for arrow_file in tqdm(arrow_files, desc="ğŸ“‚ å¤„ç†æ–‡ä»¶è¿›åº¦", unit="file"):
    input_path = os.path.join(input_folder, arrow_file)
    output_path = os.path.join(output_folder, f"processed_{arrow_file}.txt")

    print(f"\nğŸ“‚ å¼€å§‹å¤„ç†æ–‡ä»¶: {input_path}")

    try:
        dataset = load_dataset("arrow", data_files=input_path, split="train")

        if "text" in dataset.column_names:
            texts = dataset["text"]
            
            # âœ… **æ‰¹é‡å¤„ç†æ–‡æœ¬**
            processed_texts = [preprocess_text(text) for text in tqdm(texts, desc="âœ… å¤„ç†æ–‡æœ¬", unit="æ¡")]

            # âœ… **ä¿å­˜å¤„ç†åçš„æ–‡æœ¬**
            with open(output_path, "w", encoding="utf-8") as f:
                for raw, processed in zip(texts, processed_texts):
                    f.write(f"åŸå§‹æ–‡æœ¬:\n{raw}\n")
                    f.write(f"å¤„ç†åæ–‡æœ¬:\n{processed}\n")
                    f.write("=" * 100 + "\n")

            print(f"ğŸ‰ **æ–‡ä»¶ {arrow_file} å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {output_path}**")

        else:
            print(f"âš ï¸ æ–‡ä»¶ {arrow_file} ä¸­æ²¡æœ‰ 'text' åˆ—ï¼Œè·³è¿‡ï¼")

    except Exception as e:
        print(f"âŒ å¤„ç† {arrow_file} å¤±è´¥: {e}")

print(f"\nğŸ‰ **æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼**")
print(f"ğŸ“ é¢„å¤„ç†ç»“æœå·²å­˜æ”¾åœ¨: {output_folder}")
