import os
import re
import math
import ssl
import nltk
from collections import defaultdict, Counter
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
from tqdm import tqdm  # âœ… è¿›åº¦æ¡åº“
import gc  # âœ… å¼ºåˆ¶é‡Šæ”¾å†…å­˜

# âœ… è§£å†³ SSL è¯ä¹¦é—®é¢˜
ssl._create_default_https_context = ssl._create_unverified_context

# âœ… ä¸‹è½½ NLTK èµ„æºï¼ˆé¿å…æŠ¥é”™ï¼‰
nltk.download("stopwords")

# âœ… ç›®æ ‡æ–‡ä»¶å¤¹
input_folder = "/Users/wangeiden/Desktop/wikipedia_processed"
output_folder = "/Users/wangeiden/Desktop/tfidf"  # âœ… ç»“æœä¿å­˜ç›®å½•
os.makedirs(output_folder, exist_ok=True)  # âœ… ç¡®ä¿ç›®å½•å­˜åœ¨

# âœ… **åŠ è½½å¹¶å¤„ç†å•ä¸ªæ–‡ä»¶**
def process_single_file(file_path):
    """ è¯»å–å•ä¸ªæ–‡ä»¶å¹¶è®¡ç®— TF-IDF """
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read().lower()  # ä¸€æ¬¡æ€§è¯»å–æ•´ä¸ªæ–‡ä»¶ï¼Œæé«˜ I/O é€Ÿåº¦
            text = re.sub(r'\*\*\* START OF .*? \*\*\*', '', text)
            text = re.sub(r'\*\*\* END OF .*? \*\*\*', '', text)
            words = re.findall(r'\b[a-z]{3,}\b', text)  # æå–è‡³å°‘ 3 ä¸ªå­—æ¯çš„å•è¯
    except Exception as e:
        print(f"âš ï¸ è¯»å– {file_path} å¤±è´¥: {e}")
        return None
    
    if not words:
        print(f"âš ï¸ æ–‡ä»¶ {file_path} æ²¡æœ‰æœ‰æ•ˆå•è¯ï¼Œè·³è¿‡å¤„ç†ã€‚")
        return None

    # âœ… è®¡ç®— TF
    word_count = Counter(words)
    total_words = len(words)
    tf_map = {word: count / total_words for word, count in word_count.items()}

    return words, tf_map, word_count

# âœ… **è®¡ç®— DF**
def compute_df(df_map, words):
    """ æ›´æ–° DF ç»Ÿè®¡ """
    for word in set(words):
        df_map[word] += 1

# âœ… **è®¡ç®— IDF**
def compute_idf(df_map, num_docs):
    return {word: math.log((num_docs + 1) / (df + 1)) + 2.0 for word, df in df_map.items()}

# âœ… **è®¡ç®— TF-IDF**
def compute_tfidf(tf_map, idf_map):
    return {word: tf_map[word] * idf_map.get(word, 0) for word in tf_map}

# âœ… **è¿‡æ»¤ä½ TF-IDF è¯**
def filter_low_tfidf(tfidf_map, min_score=0.0001):
    return {word: score for word, score in tfidf_map.items() if score >= min_score}

# âœ… **è·å–åœç”¨è¯**
def get_stopwords(word_count, total_words):
    nltk_stopwords = set(stopwords.words("english"))
    sklearn_stopwords = set(ENGLISH_STOP_WORDS)
    auto_high_freq_stopwords = {word for word, count in word_count.items() if count > 0.02 * total_words}
    custom_stopwords = {
        "said", "one", "would", "could", "like", "got", "went", "well", "voice", "work", "works",
        "must", "know", "little", "see", "may", "yet", "also", "upon", "way", "even",
        "another", "thing", "two", "three", "many", "first", "last", "every", "though",
        "began", "white", "time", "back", "good", "hand", "head", "make", "made",
        "look", "long", "came", "day", "thought", "give", "just", "think", "let",
        "sure", "right", "left", "around", "really", "next", "come", "put"
    }
    return nltk_stopwords.union(sklearn_stopwords, auto_high_freq_stopwords, custom_stopwords)

# âœ… **è¿‡æ»¤åœç”¨è¯**
def filter_stopwords(tfidf_map, stopword_list):
    return {word: score for word, score in tfidf_map.items() if word.lower() not in stopword_list}

# âœ… **ä¸»å‡½æ•°**
def main():
    print("ğŸ“‚ æ­£åœ¨é€ä¸ªæ–‡ä»¶å¤„ç†æ–‡æœ¬...")

    file_list = sorted([f for f in os.listdir(input_folder) if f.endswith(".txt")])
    if not file_list:
        print("âš ï¸ ç›®å½•ä¸ºç©ºï¼Œæ²¡æœ‰æ‰¾åˆ° `.txt` æ–‡ä»¶ï¼")
        return

    num_docs = 0  # ç»Ÿè®¡æ€»æ–‡æ¡£æ•°
    df_map = defaultdict(int)  # è®°å½• DF

    # âœ… **ç¬¬ä¸€ééå†æ‰€æœ‰æ–‡ä»¶ï¼Œç»Ÿè®¡ DF**
    for filename in tqdm(file_list, desc="ğŸ“Š ç»Ÿè®¡ DF", unit="file"):
        file_path = os.path.join(input_folder, filename)
        result = process_single_file(file_path)
        if result:
            words, _, _ = result
            compute_df(df_map, words)
            num_docs += 1
        
        gc.collect()  # âœ… å¼ºåˆ¶é‡Šæ”¾å†…å­˜ï¼Œé¿å…å†…å­˜æ³„æ¼

    # âœ… **è®¡ç®— IDF**
    idf_map = compute_idf(df_map, num_docs)

    # âœ… **ç¬¬äºŒééå†æ‰€æœ‰æ–‡ä»¶ï¼Œè®¡ç®— TF-IDF å¹¶å†™å…¥æ–‡ä»¶**
    for filename in tqdm(file_list, desc="ğŸ“Š è®¡ç®— TF-IDF", unit="file"):
        file_path = os.path.join(input_folder, filename)
        result = process_single_file(file_path)
        if not result:
            continue

        words, tf_map, word_count = result
        tfidf_map = compute_tfidf(tf_map, idf_map)

        # âœ… **è·å–å¹¶åº”ç”¨åœç”¨è¯**
        stopword_list = get_stopwords(word_count, len(words))
        tfidf_map = filter_stopwords(filter_low_tfidf(tfidf_map), stopword_list)

        # âœ… **æ’åº TF-IDF ç»“æœ**
        sorted_tfidf = sorted(tfidf_map.items(), key=lambda x: x[1], reverse=True)

        # âœ… **æ‰“å°æ‰€æœ‰ TF-IDF è¯**
        print(f"\nğŸ“„ æ–‡ä»¶ {filename} å¤„ç†å®Œæˆï¼Œæ‰€æœ‰ TF-IDF è¯ï¼š")
        for word, score in sorted_tfidf:
            print(f"{word}: {score:.6f}")  # âœ… æ›´é«˜ç²¾åº¦

        # âœ… **ä¿å­˜å®Œæ•´ TF-IDF ç»“æœåˆ°æ–‡ä»¶**
        output_path = os.path.join(output_folder, f"{filename}_tfidf.txt")
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(f"ğŸ“„ æ–‡ä»¶å: {filename}\n")
            f.write(f"ğŸ“Š å¤„ç†å TF-IDF å…³é”®è¯:\n")
            for word, score in sorted_tfidf:
                f.write(f"{word}: {score:.6f}\n")  # âœ… ä¿æŒ 6 ä½å°æ•°ç²¾åº¦

        print(f"âœ… ç»“æœå·²å®Œæ•´ä¿å­˜è‡³: {output_path}")

        # âœ… **é‡Šæ”¾å½“å‰æ–‡ä»¶çš„å˜é‡**
        del words, tf_map, word_count, tfidf_map
        gc.collect()

    print("\nğŸ‰ **æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼**")

# âœ… **è¿è¡Œç¨‹åº**
if __name__ == "__main__":
    main()
