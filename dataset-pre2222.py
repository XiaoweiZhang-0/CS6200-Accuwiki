import os
import re
import ssl
import nltk
from datasets import load_dataset
from tqdm import tqdm
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# âœ… è§£å†³ SSL è¯ä¹¦é—®é¢˜
ssl._create_default_https_context = ssl._create_unverified_context

# âœ… ä¸‹è½½ NLTK èµ„æºï¼ˆç¡®ä¿æ²¡æœ‰ç¼ºå¤±ï¼‰
nltk.download('stopwords')
nltk.download('wordnet')

# âœ… ç›´æ¥æŒ‡å®šè·¯å¾„
input_folder = "/Users/wangeiden/Desktop/wikipedia_"
output_folder = "/Users/wangeiden/Desktop/wikipedia_processed/"

# âœ… åˆ›å»ºå­˜å‚¨ç›®å½•
os.makedirs(output_folder, exist_ok=True)

# âœ… åˆå§‹åŒ–å·¥å…·
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))  # âœ… é¢„å¤„ç†åœç”¨è¯ï¼ˆç”¨ `set()` æé«˜æŸ¥æ‰¾é€Ÿåº¦ï¼‰

# âœ… **æé€Ÿ `preprocess_text()`**
def preprocess_text(text):
    text = text.lower()  # ğŸ”¹ è½¬å°å†™
    text = re.sub(r"[^\w\s]", "", text)  # ğŸ”¹ å»æ ‡ç‚¹

    words = text.split()  # ğŸ”¹ **æ¯” `word_tokenize()` æ›´å¿«**
    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]  # ğŸ”¹ **æ‰¹é‡ Lemmatization + å»åœç”¨è¯**

    return " ".join(words)  # ğŸ”¹ **æœ€å¿«æ‹¼æ¥**

# âœ… **è·å–æ‰€æœ‰ `.arrow` æ–‡ä»¶**
arrow_files = [f for f in os.listdir(input_folder) if f.endswith(".arrow")]
if not arrow_files:
    raise ValueError(f"ç›®å½• {input_folder} ä¸­æ²¡æœ‰ Arrow æ–‡ä»¶ï¼")

# âœ… **é€ä¸ªå¤„ç†æ¯ä¸ª `.arrow` æ–‡ä»¶**
for arrow_file in tqdm(arrow_files, desc="ğŸ“‚ å¤„ç†æ–‡ä»¶è¿›åº¦", unit="file"):
    input_path = os.path.join(input_folder, arrow_file)
    output_path = os.path.join(output_folder, f"processed_{arrow_file}.txt")

    print(f"\nğŸ“‚ å¼€å§‹å¤„ç†æ–‡ä»¶: {input_path}")

    try:
        dataset = load_dataset("arrow", data_files=input_path, split="train")

        if "text" in dataset.column_names:
            texts = dataset["text"]
            
            # âœ… **ğŸ”¥ é«˜é€Ÿæ‰¹é‡å¤„ç†æ–‡æœ¬**
            processed_texts = [preprocess_text(text) for text in tqdm(texts, desc="âœ… å¤„ç†æ–‡æœ¬", unit="æ¡")]

            # âœ… **ä»…ä¿å­˜å¤„ç†åçš„æ–‡æœ¬**
            with open(output_path, "w", encoding="utf-8") as f:
                f.write("\n".join(processed_texts))

            print(f"ğŸ‰ **æ–‡ä»¶ {arrow_file} å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {output_path}**")

        else:
            print(f"âš ï¸ æ–‡ä»¶ {arrow_file} ä¸­æ²¡æœ‰ 'text' åˆ—ï¼Œè·³è¿‡ï¼")

    except Exception as e:
        print(f"âŒ å¤„ç† {arrow_file} å¤±è´¥: {e}")

print(f"\nğŸ‰ **æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼**")
print(f"ğŸ“ é¢„å¤„ç†ç»“æœå·²å­˜æ”¾åœ¨: {output_folder}")
